# this is a reusable workflow that should be used in every <client>-pipeline repository
# refer to https://docs.github.com/en/actions/using-workflows/reusing-workflows
name: Deploy Pipeline
on:
  workflow_call:
    inputs:
      environment:
        description: The target environment to deploy to. prod, stg, qa, int, or a development environment
        required: true
        type: string
      pipeline_name:
        description: The name of the pipeline flow file e.g. for "pipeline_main.py," input "main"
        required: false
        type: string
        default: main
      awsAccountId:
        description: The client's AWS account ID
        required: false # defaults to repo secret AWS_ACCOUNT_ID
        type: string
      awsRegion:
        description: The client's AWS account region e.g. us-east-1, eu-west-2, etc
        required: false # defaults to repo secret AWS_REGION
        type: string
      clientCode:
        description: The lowercase Datateer client code e.g. pkt, hmn, etc
        required: false # defaults to repo secret CLIENT_CODE
        type: string
      datateerCliVersion:
        description: Datateer CLI version. Leave blank to use the latest version. See https://pypi.org/project/datateer-cli/
        type: string
    secrets:
      deployKeyPrefectLib:
        description: The github deployment key that allows access to Datateer/datateer-prefect. This is available as an organization secret
        required: true
      deployKeyPrefect:
        description: The Prefect deployment key for this client's Prefect project
        required: true
      deploymentAgentAwsAccessKey:
        description: The AWS Access Key ID for the client's deployment agent. This should be stored as a repo secret in <client>-pipeline
        required: true
      deploymentAgentAwsAccessKeySecret: 
        description: The AWS Access Key Secret for the client's deployment agent. This should be stored as a repo secret in <client>-pipeline
        required: true
        
jobs:
  deploy:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8]
    name: Build and deploy the pipeline
    env:
      AWS_ACCOUNT_ID: ${{ inputs.awsAccountId || secrets.AWS_ACCOUNT_ID }}
      AWS_REGION: ${{ inputs.awsRegion || secrets.AWS_REGION }}
      CLIENT_CODE: ${{ inputs.clientCode || secrets.CLIENT_CODE }}
      DATATEER_ENV: ${{ inputs.environment || 'int' }}
      PIPELINE_NAME: ${{ inputs.pipeline_name || 'main' }}
    steps:
      - name: Prerequisites
        run: |
          [ -n "${{ env.AWS_ACCOUNT_ID }}" ] || (echo "Could not find AWS_ACCOUNT_ID in the inputs or in environment variables" && exit 1)
          [ -n "${{ env.AWS_REGION }}" ] || (echo "Could not find AWS_REGION in the inputs or in environment variables" && exit 1)
          [ -n "${{ env.CLIENT_CODE}}" ] || (echo "Could not find CLIENT_CODE in the inputs or in environment variables" && exit 1)
          [ -n "${{ env.DATATEER_ENV }}" ] || (echo "Could not find DATATEER_ENV in the inputs or in environment variables" && exit 1)
          
          python -m pip install --upgrade pip
          
          sudo apt-get install graphviz graphviz-dev
          if [ -n "${{ inputs.datateerCliVersion }}" ]
          then
            pip install datateer-cli==${{ inputs.datateerCliVersion }}
          else
            pip install datateer-cli
          fi
        
      # This has a serious limitation that makes it difficult to use beyond a single SSH key. Ref https://github.com/webfactory/ssh-agent#using-multiple-keys
      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.4.1
        with:
          ssh-private-key: |
            ${{ secrets.deployKeyPrefectLib  }}

      - name: Checkout pipeline repo
        uses: actions/checkout@v2

      - uses: syphar/restore-pip-download-cache@v1
      - run: |
          chmod 755 ./.datateer/build_scripts/pre-build.sh
          ./.datateer/build_scripts/pre-build.sh
          pip install .

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.deploymentAgentAwsAccessKey }}
          aws-secret-access-key: ${{ secrets.deploymentAgentAwsAccessKeySecret }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Pull down config settings files
        run: datateer config pull --environment ${{ env.DATATEER_ENV }}

      # The step that builds the docker image needs access to the meltano database. The next 2 steps dynamically open and close ports allowing github to access the AWS RDS database
      - name: Get AWS security group ID for the meltano database
        id: security_group_id_step
        run: |
          VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=default --query "Vpcs[].VpcId" --output text)
          SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filter Name=vpc-id,Values=$VPC_ID Name=group-name,Values=meltano-db-sg --query 'SecurityGroups[*].[GroupId]' --output text)
          echo "::set-output name=security_group_id::$SECURITY_GROUP_ID"

      # If meltano is not in use, then then the security group ID will be empty and this step will not run
      - name: Add public IP to AWS security group so that meltano can deploy to the database
        if: ${{ null != steps.security_group_id_step.outputs.security_group_id  }}
        uses: sohelamin/aws-security-group-add-ip-action@master
        with:
          aws-access-key-id: ${{ secrets.deploymentAgentAwsAccessKey  }}
          aws-secret-access-key: ${{ secrets.deploymentAgentAwsAccessKeySecret  }}
          aws-region: ${{ env.AWS_REGION }}
          aws-security-group-id: ${{ steps.security_group_id_step.outputs.security_group_id }}
          port: "5432"
          to-port: "5432"
          protocol: "tcp"
          description: "Created by a GitHub Action - will be deleted when finished"

      - name: Load docker layer cache
        uses: satackey/action-docker-layer-caching@v0.0.11
        # Ignore the failure of a step and avoid terminating the job.
        continue-on-error: true
        with:
          key: datateer-docker-pipeline-${{ env.CLIENT_CODE }}-${{ env.DATATEER_ENV }}-{hash}
          restore-keys: |
            datateer-docker-pipeline-${{ env.CLIENT_CODE }}-${{ env.DATATEER_ENV }}-
            datateer-docker-pipeline-${{ env.CLIENT_CODE }}-
            datateer-docker-pipeline-

      - name: Deploy pipeline
        run: datateer pipeline deploy main --environment ${{ env.DATATEER_ENV }}
        env:
          DEPLOY_KEY_PREFECT_LIB: ${{ secrets.deployKeyPrefectLib }}
          PREFECT__CLOUD__API_KEY: ${{ secrets.deployKeyPrefect }}
          AWS_ACCESS_KEY_ID: ${{ secrets.deploymentAgentAwsAccessKey }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.deploymentAgentAwsAccessKeySecret }}

      - name: Deploy documentation
        run: datateer docs deploy -c ${{ env.CLIENT_CODE }} {{ env.PIPELINE_NAME }}
